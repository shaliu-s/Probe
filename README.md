# Probe related papers in hallucination detection (order by date)

- LLM Internal States Reveal Hallucination Risk Faced With a Query

- The Internal State of an LLM Knows When Itâ€™s Lying

- Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs

- Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?

- Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models

- STILL NO LIE DETECTOR FOR LANGUAGE MODELS: PROBING EMPIRICAL AND CONCEPTUAL ROADBLOCKS

- Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation

- Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators

- Understanding intermediate layers using linear classifier probes

- <font color="green">Probing classifiers: Promises, shortcomings, and advances</font>

- <font color="green">The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets. </font>

- <font color="green">DISCOVERING LATENT KNOWLEDGE IN LANGUAGE MODELS WITHOUT SUPERVISION</font> 
