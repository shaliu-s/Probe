# Probe related papers in hallucination detection (order by date)


- LLM Internal States Reveal Hallucination Risk Faced With a Query.  **(2024.07 | ðŸŸ© | HKUST)**

- Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators.  **(2024.07 | ACL 2024 | AWS AI Labs)**

- Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs.  **(2024.06 | ðŸŸ© | University of Oxford)**

- Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?.  **(2024.04 | ðŸŸ© | Rutgers University)**

- Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models.  **(2024.03 | ACL findings 2024 | Tsinghua University)**
  
- Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation.  **(2024.01 | ACL Workshop 2024 | X2Robot)**

- The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets.  **(2023.10 | COLM 2024 | Northeastern University\MIT)**

- STILL NO LIE DETECTOR FOR LANGUAGE MODELS: PROBING EMPIRICAL AND CONCEPTUAL ROADBLOCKS.  **(2023.06 | Philosophical Studies | UIUC)**

- The Internal State of an LLM Knows When Itâ€™s Lying.  **(2023.04 | EMNLP findings 2023| Ariel University)**
  
- DISCOVERING LATENT KNOWLEDGE IN LANGUAGE MODELS WITHOUT SUPERVISION.  **(2022.12 | ICLR 2023 | UC Berkeley\Peking University)**

- Probing classifiers: Promises, shortcomings, and advances.  **(2021.09 | Technion â€“ Israel Institute of Technology)**
  
- Understanding intermediate layers using linear classifier probes.  **(2018.11 | University of Montreal)**


