# Probe related papers in hallucination detection

- <font color="green">LLM Internal States Reveal Hallucination Risk Faced With a Query </font>

- <font color="green">The Internal State of an LLM Knows When Itâ€™s Lying</font> 

- <font color="green">Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs </font>

- <font color="green">Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?</font>

- <font color="green">Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models</font> 

- <font color="green">STILL NO LIE DETECTOR FOR LANGUAGE MODELS: PROBING EMPIRICAL AND CONCEPTUAL ROADBLOCKS </font>

- <font color="green">Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation</font>

- <font color="green">Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators </font>

- <font color="green">Understanding intermediate layers using linear classifier probes</font> 

- <font color="green">Probing classifiers: Promises, shortcomings, and advances</font>

- <font color="green">The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets. </font>

- <font color="green">DISCOVERING LATENT KNOWLEDGE IN LANGUAGE MODELS WITHOUT SUPERVISION</font> 
